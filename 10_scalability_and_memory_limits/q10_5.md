## Question 10.5
If you were designing a web crawler, 

how would you avoid getting into infinite loops?

## Solution

1. 起点となるページのURLを読み込み
1. ページの URL(v) の hash(v) をストレージに保存する
    * 過去に巡回していたページを巡回しない
1. ページの html ファイルの body 配下で記述されているリンクを順に巡回する。次のページに移動する際は、前のページの URL(prev_url) を保持しておく.
1. もしページ内にリンクが全て訪問済みもしくはリンクがない場合、 prev_urlをの処理に戻る

## Solution (Text)

ページの類似性をある程度推定する方法として、下記の項目からシグネチャを作成し、判断する
* URL
* ページのコンテンツ

1. ページを開き, 内容の一部と URL を元にシグネチャを作成
1. 直近にアクセスが有ったかどうかを確認するため、ストレージに問い合わせ
1. シグネチャと類似度が近いページへのアクセスが有った場合、そのページの優先順位を下げてデータベースに保存
1. そうでなければそのページを巡回し、巡回後にストレージに新たにレコードを追加する

## Question 10.6

You have 10 billion URLs. How do you detect the duplicate documents?

In this case, assume that "duplicate" means that the URLs are identical.

## Solution

* 10 billion = 10,000,000,000
* 1 URL = "https://www.google.com/" = 23 chars < 50 chars => 50 Byte
* 10 billion URLs = 500 GB 

1. メモリに乗せることが可能な URL の数を産出
2. ファイルの先頭より、 1. で求めた数のURLを DataBlock として HashSet を作成する.
   追加時に重複した値を `.contains()` method などで確認し、重複が見つかった場合はその URL を出力して終了する
3. ファイルの残りのデータを stream で loop 処理し、 2. で作った HashSet 内に URLが存在するか確認する.
4. もし 3. の中で重複が見つからんなかった場合、 2. に戻り次の DataBlock を作成する.

## Solution (Text)

前提: 1GB RAM にデータを格納して走査するものとする.

1. 500GB の URLs を 1GB のメモリで処理できるようにするため、 剰余500 で hash値を求める. 
```python
HASH = 500

with open("URLs.txt") as urls:
   url = urls.read()
   h = url % HASH
   out = open(f"intermediate_{h}.txt", "w")
   out.write(url)
```

2. 1. で作成した中間ファイル内で重複がないか メモリにデータを展開して確認する